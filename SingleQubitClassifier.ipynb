{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Single qubit classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR-wvRNYHz66"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install pennylane\n",
        "\n",
        "# Download of the required DataFactory and Logger classes from GitHub.\n",
        "!rm Logger.py\n",
        "!wget -O Logger.py https://raw.githubusercontent.com/LorenzoFioroni/bachelor-thesis/main/Logger.py\n",
        "!rm DataFactory.py\n",
        "!wget -O DataFactory.py https://raw.githubusercontent.com/LorenzoFioroni/bachelor-thesis/main/DataFactory.py"
      ],
      "id": "oR-wvRNYHz66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dca2091f-514f-4289-8f3d-a7458bc697b1"
      },
      "source": [
        "# Baseline\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Utility\n",
        "import DataFactory as factory   # Custom module for data generation\n",
        "from Logger import Logger       # Custom module for data logging"
      ],
      "id": "dca2091f-514f-4289-8f3d-a7458bc697b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "3fc33cac-c4a6-4229-a286-9f3a7dfe8fd5",
        "outputId": "3d124493-fb32-4615-c211-2b6e79568acb"
      },
      "source": [
        "# Plot of the available 2D dataSets\n",
        "factory.plot2D()"
      ],
      "id": "3fc33cac-c4a6-4229-a286-9f3a7dfe8fd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPuxwk6wgGl6"
      },
      "source": [
        "# Logic"
      ],
      "id": "tPuxwk6wgGl6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7094b24-6f49-442f-84d7-a4463ddb18d9"
      },
      "source": [
        "# This cell contains the logic for the qubit simulation. The \"circuit\" function\n",
        "# takes as input the coordinates x of the point to be classified, the matrix\n",
        "# y relative to the labelState for the measurement and the parameters needed during\n",
        "# the process. Then for each layer, the \"processingLayer\" function is called which \n",
        "# splits the data in chunks with size 3 and applies the rotations to the qubit.\n",
        "\n",
        "device = qml.device(\"default.qubit\", wires=1)\n",
        "\n",
        "# Performs the weighted data re-uploading process\n",
        "def processingLayer(x, thetaRow, alphaRow, nChunks):\n",
        "  for chunk, alphaChunk, thetaChunk in zip( np.array_split(x, nChunks), np.array_split(alphaRow, nChunks), np.array_split(thetaRow, nChunks) ):\n",
        "    padding = [0]*(3-len(chunk))\n",
        "    qml.Rot(*(chunk*alphaChunk), *padding, wires=0)\n",
        "    qml.Rot(*thetaChunk, wires=0)\n",
        "\n",
        "\n",
        "# Simulates the classification process for the given point and with the given\n",
        "# parameters. Returns |<Ψ_class|Ψ_computed>|^2 \n",
        "@qml.qnode(device, interface=\"torch\")\n",
        "def circuit(x, y, theta, alpha):\n",
        "  nChunks = 1 + (len(x)-1) // 3\n",
        "  for thetaRow, alphaRow in zip(theta, alpha):\n",
        "    processingLayer(x, thetaRow, alphaRow, nChunks)\n",
        "  return qml.expval(qml.Hermitian(y, wires=0))"
      ],
      "id": "e7094b24-6f49-442f-84d7-a4463ddb18d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68622c8a-2acd-4863-aefb-d2b30651fd93"
      },
      "source": [
        "# Function to compute the cassification cost for the given tensor of points\n",
        "def cost(params, labelStates, expectedFidelity, x_batch, y_batch):\n",
        "  loss = 0\n",
        "  for x, y in zip(x_batch, y_batch):\n",
        "    loss += (1 - circuit(x, labelStates[y], params[0], params[1]))\n",
        "  return loss/len(x_batch)"
      ],
      "id": "68622c8a-2acd-4863-aefb-d2b30651fd93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20fb010f-303b-4d20-ad0f-7ee942f3485e"
      },
      "source": [
        "# Function to compute the weighted cassification cost for the given tensor of points\n",
        "def weightedCost(params, labelStates, expectedFidelity, x_batch, y_batch):\n",
        "  loss = 0\n",
        "  for x, y in zip(x_batch, y_batch):\n",
        "    for c in range(len(labelStates)):\n",
        "      loss += (params[2][c] * circuit(x, labelStates[c], params[0], params[1]) - expectedFidelity[y, c])**2\n",
        "        \n",
        "  return 0.5*loss/len(x_batch)"
      ],
      "id": "20fb010f-303b-4d20-ad0f-7ee942f3485e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0b0c78f-cc07-4d28-a5c6-a22825d288dc"
      },
      "source": [
        "# Function to perform the parameters update\n",
        "def closure(x_batch, y_batch, params, labelStates, expectedFidelity, opt, costFunction):\n",
        "  opt.zero_grad()\n",
        "  loss = costFunction(params, labelStates, expectedFidelity, x_batch, y_batch)\n",
        "  loss.backward()\n",
        "  return loss"
      ],
      "id": "c0b0c78f-cc07-4d28-a5c6-a22825d288dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba351d59-bd1c-4b01-a2a7-7dfac42cbcbc"
      },
      "source": [
        "# Function to predict the class for the given thensor of points. For each class the \n",
        "# classification process is performed. The assigned class is the one whose labelState \n",
        "# has the maximum overlap with the state |Ψ> after the classification. If classWeights \n",
        "# are provided, each overlap is weighted according to them. \n",
        "def predict(x, params, labelStates):\n",
        "  predicted = torch.empty(len(x))\n",
        "  predictions = torch.empty(len(labelStates))\n",
        "  \n",
        "  for i in range(len(x)):\n",
        "    for j in range(len(labelStates)):\n",
        "      predictions[j] = circuit(x[i], labelStates[j], params[0], params[1])\n",
        "\n",
        "    if len(params) == 3: # classWeights\n",
        "      predicted[i] = torch.argmax(params[2]*predictions)\n",
        "    else: # No classWeights\n",
        "      predicted[i] = torch.argmax(predictions)\n",
        "      \n",
        "  return predicted"
      ],
      "id": "ba351d59-bd1c-4b01-a2a7-7dfac42cbcbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af58f970-3ad0-42e2-8900-17230f57ec5c"
      },
      "source": [
        "# Function to plot the given tensor of points for a 2D problem with the colors\n",
        "# specified in the tensor \"y\". The boundaries between two classes in the domain \n",
        "# are plotted dashed below the points. The default option is to show the plot and\n",
        "# concurrently to save it with the provided name.\n",
        "def plotDataSet(name, title, x, y, dataSet, logger, toTerminal = True, toFile = True):\n",
        "\n",
        "  if dataSet.dim != 2: return\n",
        "  \n",
        "  x_shape, y_shape = dataSet.getShape()\n",
        "  \n",
        "  f = plt.figure(figsize=(3.5, 3.5))\n",
        "  ax = f.add_subplot()\n",
        "  ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "  for i in range(len(x_shape)):\n",
        "    plt.plot(x_shape[i], y_shape[i], lw=3, color=\"r\", ls=\"dashed\")\n",
        "  plt.scatter(x[:,0], x[:,1], c=y, s = 6, cmap=\"Accent\") \n",
        "\n",
        "  plt.xlim(-1,1)\n",
        "  plt.ylim(-1,1)\n",
        "  plt.title(\"{}\\n{}\".format(dataSet.name,title))\n",
        "\n",
        "  plt.tight_layout()\n",
        "  if toFile: logger.savePlot(plt, name)\n",
        "  if toTerminal: plt.show()\n",
        "  \n",
        "  plt.close()"
      ],
      "id": "af58f970-3ad0-42e2-8900-17230f57ec5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fe489f4-2829-430e-893e-c8cde5ac2bb5"
      },
      "source": [
        "# Function to generate a simple plot showing the loss and accuracy evolutions \n",
        "# vs. the epoch of training.\n",
        "def plotMetrics(metrics, logger):\n",
        "\n",
        "  valid_present = metrics.shape[1] == 5\n",
        "\n",
        "  print(\"\")\n",
        "\n",
        "  f = plt.figure(figsize=(12, 5))\n",
        "  gs = f.add_gridspec(1, 2)\n",
        "  \n",
        "  ax = f.add_subplot(gs[0,0])\n",
        "  plt.plot(metrics[:, 0], metrics[:, 1], label=\"training set\")\n",
        "  if valid_present: plt.plot(metrics[:, 0], metrics[:, 3], label=\"validation set\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.xlabel(\"# of epochs\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Loss\")\n",
        "\n",
        "  ax = f.add_subplot(gs[0,1])\n",
        "  plt.plot(metrics[:, 0], metrics[:, 2], label=\"training set\")\n",
        "  if valid_present: plt.plot(metrics[:, 0], metrics[:,4], label=\"validation set\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.ylim(0, 1)\n",
        "  plt.xlabel(\"# of epochs\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Accuracy\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  logger.savePlot(plt, \"metrics.jpg\")\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "id": "7fe489f4-2829-430e-893e-c8cde5ac2bb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ab2698-f65e-4e04-bfe2-8db859525163"
      },
      "source": [
        "# Function to perform nEpochs epochs of training. Periodically plots the training set with \n",
        "# the predicted labels and saves it to file. If a validation set is provided, the parameters \n",
        "# which have performed the best on it are restored at the end of the training process\n",
        "# Loss and accuracy are saved each epoch in order to plot them at the end of the simulation. \n",
        "def train(dataSet, opt, batchSize, lrDecay, nEpochs, params, costFunction, logger):\n",
        "    \n",
        "  labelStates, expectedFidelity = dataSet.getStates()\n",
        "  x_train, y_train = dataSet.getTrain()\n",
        "  x_valid, y_valid = dataSet.getValid()\n",
        "  valid_present = len(x_valid)!=0 # Whether or not a validation set is present\n",
        "\n",
        "  lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=0.96)\n",
        "\n",
        "  totEpochs = 0 # Epoch index\n",
        "  \n",
        "  nBatches =  len(x_train) // batchSize + int(len(x_train)%batchSize != 0)\n",
        "\n",
        "  metrics = torch.empty(1,3 + 2*valid_present)\n",
        "\n",
        "  header = [\"loss\", \"accuracy\", \"valid_loss\", \"valid_accuracy\"]\n",
        "  logger.log((\"epoch\"+\"\\t{}\"*(2+2*valid_present)).format(*header[:2*(1+valid_present)]))\n",
        "\n",
        "  logFormat = \"{: >5d}\\t{: >4.3f}\\t{: >8.3f}\"\n",
        "  if valid_present: logFormat += \"\\t{: >10.3f}\\t{: >14.3f}\"\n",
        "\n",
        "  # Metrics at epoch 0\n",
        "  loss = costFunction(params, labelStates, expectedFidelity, x_train,y_train)\n",
        "  accuracy = accuracy_score(y_train, predict(x_train, params, labelStates))\n",
        "  metrics_row = [0, loss, accuracy]\n",
        "\n",
        "  if valid_present:\n",
        "    val_loss = costFunction(params, labelStates, expectedFidelity, x_valid,y_valid)\n",
        "    val_accuracy = accuracy_score(y_valid, predict(x_valid, params, labelStates))\n",
        "    metrics_row. extend([val_loss, val_accuracy])\n",
        "    \n",
        "    bestParams = params\n",
        "    bestLoss = val_loss\n",
        "    bestEpoch = 0\n",
        "\n",
        "  metrics[0, :] = torch.tensor(metrics_row)\n",
        "\n",
        "  logger.log(logFormat.format(*metrics_row))\n",
        "\n",
        "  for epoch in range(nEpochs):\n",
        "    p = torch.randperm(len(x_train))\n",
        "    x_train = x_train[p]\n",
        "    y_train = y_train[p]\n",
        "\n",
        "    # Training the circuit with batched data\n",
        "    for x_batch, y_batch in zip( np.array_split(x_train, nBatches), np.array_split(y_train, nBatches) ):\n",
        "      opt.step(lambda: closure(x_batch, y_batch, params, labelStates, expectedFidelity, opt, costFunction))\n",
        "\n",
        "    totEpochs += 1\n",
        "\n",
        "    # Learning rate decay\n",
        "    if totEpochs > 5: \n",
        "      lr_scheduler.step()\n",
        "\n",
        "    # Metrics at epoch totEpochs\n",
        "    loss = costFunction(params, labelStates, expectedFidelity, x_train,y_train)\n",
        "    accuracy = accuracy_score(y_train, predict(x_train, params, labelStates))\n",
        "    metrics_row = [totEpochs, loss, accuracy]\n",
        "\n",
        "    if valid_present:\n",
        "      val_loss = costFunction(params, labelStates, expectedFidelity, x_valid,y_valid)\n",
        "      val_accuracy = accuracy_score(y_valid, predict(x_valid, params, labelStates))\n",
        "      metrics_row.extend([val_loss, val_accuracy])\n",
        "\n",
        "      if val_loss <= bestLoss:\n",
        "        bestLoss = val_loss\n",
        "        bestParams = params\n",
        "        bestEpoch = totEpochs\n",
        "\n",
        "    metrics = torch.cat((metrics, torch.tensor([metrics_row])), 0)\n",
        "\n",
        "    logger.log(logFormat.format(*metrics_row))\n",
        "\n",
        "    # Plotting the training set every 5 epochs\n",
        "    if totEpochs % 5 == 0:\n",
        "      plotDataSet(\n",
        "        \"{}_epochs.jpg\".format(totEpochs),\n",
        "        \"{} epochs\".format(totEpochs),\n",
        "        x_train,\n",
        "        predict(x_train, params, labelStates),\n",
        "        dataSet,\n",
        "        logger,\n",
        "        toTerminal=False\n",
        "      )\n",
        "\n",
        "  \n",
        "  # Last parameters in case of no validation set, best parameters otherwise \n",
        "  return ( (bestParams, bestEpoch, metrics) if valid_present else (params, totEpochs, metrics) )"
      ],
      "id": "e9ab2698-f65e-4e04-bfe2-8db859525163",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da8df352-c829-44e3-9caa-352061ed4611"
      },
      "source": [
        "# Simulator"
      ],
      "id": "da8df352-c829-44e3-9caa-352061ed4611"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e568283b-3f31-4053-b6ca-578e03c3021d"
      },
      "source": [
        "# Available dataSets. Specify which one to use in the cell below. The plots will be generated for 2D dataSets only\n",
        "dataSetDict = {\n",
        "  \"circle\": factory.Circle,                                    # 2D - 2 classes\n",
        "  \"binary annulus\": factory.BinaryAnnulus,                     # 2D - 2 classes\n",
        "  \"non convex\": factory.NonConvex,                             # 2D - 2 classes\n",
        "  \"annulus\": factory.Annulus,                                  # 2D - 3 classes\n",
        "  \"three circles\": factory.ThreeCircles,                       # 2D - 4 classes\n",
        "  \"squares\": factory.Squares,                                  # 2D - 4 classes\n",
        "  \"wavy lines\": factory.WavyLines,                             # 2D - 4 classes\n",
        "  \"sphere\": factory.Sphere,                                    # 3D - 2 classes\n",
        "  \"four dimensional hypersphere\": factory.FourDimHypersphere   # 4D - 2 classes\n",
        "\n",
        "}\n",
        "\n",
        "# Available optimizers. Specify which one to use in the cell below\n",
        "optimizerDict = {\n",
        "  \"adam\": torch.optim.Adam,\n",
        "  \"rmsprop\": torch.optim.RMSprop,\n",
        "  \"sgd\": torch.optim.SGD,\n",
        "  \"lbfgs\": torch.optim.LBFGS\n",
        "}\n",
        "\n",
        "# Available cost functions. Specify which one to use in the cell below\n",
        "costDict = {\n",
        "  \"fidelity\": cost,\n",
        "  \"weighted fidelity\": weightedCost\n",
        "}"
      ],
      "id": "e568283b-3f31-4053-b6ca-578e03c3021d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cf2da429-70b6-44e9-bab7-eb8778db6902",
        "outputId": "3c1e3583-c526-45b8-845c-21419fc0b6fd"
      },
      "source": [
        "nTrain = 500                   # Number of elements in training set\n",
        "nValid = 0                     # Number of elements in validation set\n",
        "nTest = 3000                   # Number of elements in test set\n",
        "nLayers = 4                    # Number of layers\n",
        "nEpochs = 30                   # Number of training epochs\n",
        "batchSize = 32                 # Batch size for training\n",
        "dataSetKey = \"circle\"      # Dataset to use (from the list in the cell above)\n",
        "optimizerKey = \"adam\"          # Optimizer to use (from the list in the cell above)\n",
        "optParams = {                  # Parameters given to the optimizer\n",
        "      \"lr\": 0.03,\n",
        "}\n",
        "lrDecay = 0.96                 # Learning rate decay factor\n",
        "costKey = \"weighted fidelity\"  # Cost function to use (from the list in the cell above)\n",
        "seed = None                    # Seed for reproducibility. To leave unset: None\n",
        "\n",
        "# -----------------\n",
        "\n",
        "logger = Logger(toTerminal = True, toFile = True)\n",
        "logger.log(\"Optimizer: {}\".format(optimizerKey), toTerminal = False)\n",
        "logger.log(\"Optimizer params: {}\".format(optParams), toTerminal = False)\n",
        "logger.log(\"DataSet: {} - nTrain: {} - nValid: {} - nTest: {}\".format(dataSetKey, nTrain, nValid, nTest), toTerminal = False)\n",
        "logger.log(\"nLayers: {} - nEpochs: {} - batchSize: {}\".format(nLayers, nEpochs, batchSize), toTerminal = False)\n",
        "logger.log(\"Cost: {}\".format(costKey), toTerminal = False)\n",
        "logger.log(\"Seed: {}\".format(seed), toTerminal = False)\n",
        "\n",
        "# Applying the choices for the simulation\n",
        "if seed: \n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(0)\n",
        "\n",
        "dataSet = (dataSetDict[dataSetKey])(nTrain=nTrain, nValid=nValid, nTest=nTest, seed=seed)\n",
        "\n",
        "x_train, y_train = dataSet.getTrain()\n",
        "x_test, y_test = dataSet.getTest()\n",
        "\n",
        "labelStates, expectedFidelity = dataSet.getStates()\n",
        "costFunction = costDict[costKey]\n",
        "\n",
        "# Extracting the parameters randomly from a normal distribution\n",
        "params = [\n",
        "  torch.randn(nLayers, (((dataSet.dim-1)//3)+1)*3).requires_grad_(True),  # theta\n",
        "  torch.randn(nLayers, dataSet.dim).requires_grad_(True)   # alpha\n",
        "]\n",
        "if costKey == \"weighted fidelity\":\n",
        "  params.append(torch.ones(dataSet.nClasses).requires_grad_(True))   # class weights\n",
        "\n",
        "opt = (optimizerDict[optimizerKey])(params, **optParams)\n",
        "\n",
        "# Plotting the trainin set with the correct classes and with those descending from the \n",
        "# classification with the random parameters just extracted\n",
        "plotDataSet(\"true_classes.jpg\", \"True\", x_train, y_train, dataSet, logger, toTerminal = False)\n",
        "plotDataSet(\"0_epochs.jpg\", \"0 epochs\", x_train, predict(x_train, params, labelStates), dataSet, logger)\n",
        "\n",
        "logger.separator()\n",
        "\n",
        "# Training\n",
        "params, params_epoch, metrics = train(dataSet, opt, batchSize, lrDecay, nEpochs, params, costFunction, logger)\n",
        "\n",
        "logger.separator()\n",
        "\n",
        "# Printing the parameters after the classification \n",
        "logger.log(\"Parameters at epoch {}:\\n\".format(params_epoch))\n",
        "\n",
        "logger.log(\"Theta:\")\n",
        "for row in params[0]:\n",
        "  logger.log((\"\\t\"+\"{: >7.4f}  \"*len(row)).format(*row))\n",
        "\n",
        "logger.log(\"Alpha:\")\n",
        "for row in params[1]:\n",
        "  logger.log((\"\\t\"+\"{: >7.4f}  \"*len(row)).format(*row))\n",
        "\n",
        "if costKey == \"weighted fidelity\":\n",
        "  logger.log(\"Class weight:\")\n",
        "  logger.log((\"\\t\"+\"{: >7.4f}  \"*len(params[2])).format(*params[2]))\n",
        "\n",
        "logger.separator()\n",
        "\n",
        "# Printing the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, predict(x_test, params, labelStates))\n",
        "logger.log(\"Accuracy on test set with the parameters above: {}\\n\".format(accuracy))\n",
        "\n",
        "# Plotting the test set classified with the trained parameters and\n",
        "# the evolutions of loss and accuracy\n",
        "plotDataSet(\"test.jpg\", \"Test set\", x_test, predict(x_test, params, labelStates), dataSet, logger)\n",
        "\n",
        "plotMetrics(metrics, logger)\n",
        "\n",
        "logger.close()"
      ],
      "id": "cf2da429-70b6-44e9-bab7-eb8778db6902",
      "execution_count": null,
      "outputs": []
    }
  ]
}