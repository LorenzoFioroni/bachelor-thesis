03:26:32	Optimizer: adam
03:26:32	Optimizer params: {'lr': 0.02}
03:26:32	DataSet: squares - nTrain: 500 - nValid: 0 - nTest: 3000
03:26:32	nLayers: 4 - nEpochs: 30 - batchSize: 32
03:26:32	Cost: fidelity
03:26:32	Seed: None

------------------------------

03:26:41	epoch	loss	accuracy
03:26:49	    0	0.351	   0.482
03:27:19	    1	0.252	   0.626
03:27:50	    2	0.184	   0.828
03:28:21	    3	0.142	   0.836
03:28:51	    4	0.132	   0.856
03:29:22	    5	0.127	   0.868
03:29:59	    6	0.125	   0.868
03:30:30	    7	0.123	   0.878
03:31:01	    8	0.121	   0.872
03:31:32	    9	0.119	   0.878
03:32:03	   10	0.117	   0.878
03:32:40	   11	0.115	   0.876
03:33:11	   12	0.113	   0.866
03:33:42	   13	0.110	   0.880
03:34:12	   14	0.108	   0.878
03:34:43	   15	0.106	   0.882
03:35:20	   16	0.104	   0.890
03:35:52	   17	0.102	   0.888
03:36:23	   18	0.101	   0.882
03:36:53	   19	0.100	   0.882
03:37:25	   20	0.099	   0.884
03:38:02	   21	0.098	   0.884
03:38:32	   22	0.098	   0.878
03:39:03	   23	0.097	   0.890
03:39:34	   24	0.096	   0.886
03:40:04	   25	0.096	   0.888
03:40:42	   26	0.096	   0.888
03:41:13	   27	0.095	   0.892
03:41:44	   28	0.095	   0.888
03:42:15	   29	0.095	   0.894
03:42:46	   30	0.095	   0.894

------------------------------

03:42:53	Parameters at epoch 30:

03:42:53	Theta:
03:42:53		-0.2756  -1.4592  -0.1278  
03:42:53		-1.2171  -1.4461   1.6889  
03:42:53		 1.9029  -0.5747   0.7141  
03:42:53		-0.5480  -1.6310  -2.5381  
03:42:53	Alpha:
03:42:53		-0.9841  -2.5136  
03:42:53		-2.7412   0.0127  
03:42:53		-0.6051  -2.7381  
03:42:53		 0.9734   0.5479  

------------------------------

03:43:28	Accuracy on test set with the parameters above: 0.8873333333333333

