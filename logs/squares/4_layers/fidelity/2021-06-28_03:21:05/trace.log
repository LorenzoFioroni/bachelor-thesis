03:21:05	Optimizer: adam
03:21:05	Optimizer params: {'lr': 0.02}
03:21:05	DataSet: squares - nTrain: 500 - nValid: 0 - nTest: 3000
03:21:05	nLayers: 4 - nEpochs: 30 - batchSize: 32
03:21:05	Cost: fidelity
03:21:05	Seed: None

------------------------------

03:21:10	epoch	loss	accuracy
03:21:19	    0	0.547	   0.054
03:21:47	    1	0.291	   0.400
03:22:15	    2	0.187	   0.742
03:22:43	    3	0.158	   0.792
03:23:11	    4	0.150	   0.802
03:23:39	    5	0.146	   0.820
03:24:13	    6	0.144	   0.836
03:24:41	    7	0.143	   0.842
03:25:09	    8	0.143	   0.830
03:25:37	    9	0.143	   0.846
03:26:05	   10	0.142	   0.840
03:26:40	   11	0.142	   0.848
03:27:07	   12	0.141	   0.848
03:27:36	   13	0.141	   0.838
03:28:04	   14	0.141	   0.848
03:28:32	   15	0.141	   0.844
03:29:06	   16	0.140	   0.848
03:29:34	   17	0.140	   0.846
03:30:03	   18	0.140	   0.850
03:30:31	   19	0.140	   0.842
03:30:59	   20	0.140	   0.852
03:31:33	   21	0.140	   0.854
03:32:02	   22	0.139	   0.852
03:32:30	   23	0.139	   0.856
03:32:58	   24	0.139	   0.844
03:33:26	   25	0.139	   0.860
03:34:00	   26	0.139	   0.854
03:34:28	   27	0.138	   0.854
03:34:56	   28	0.138	   0.852
03:35:25	   29	0.138	   0.854
03:35:54	   30	0.138	   0.858

------------------------------

03:36:00	Parameters at epoch 30:

03:36:00	Theta:
03:36:00		 1.7385   1.8044   0.0483  
03:36:00		 1.1618   1.1501   1.1597  
03:36:00		-0.0497   0.6715  -0.6355  
03:36:00		 0.5019   0.3041  -0.6310  
03:36:00	Alpha:
03:36:00		 1.1061  -0.5946  
03:36:00		-0.6213   1.4676  
03:36:00		-1.0135  -0.2914  
03:36:00		-1.0328   0.1260  

------------------------------

03:36:31	Accuracy on test set with the parameters above: 0.8616666666666667

