02:15:56	Optimizer: adam
02:15:56	Optimizer params: {'lr': 0.02}
02:15:56	DataSet: squares - nTrain: 500 - nValid: 0 - nTest: 3000
02:15:56	nLayers: 8 - nEpochs: 30 - batchSize: 32
02:15:56	Cost: fidelity
02:15:56	Seed: None

------------------------------

02:16:06	epoch	loss	accuracy
02:16:17	    0	0.437	   0.260
02:17:28	    1	0.188	   0.758
02:18:39	    2	0.161	   0.754
02:19:51	    3	0.143	   0.802
02:21:02	    4	0.122	   0.842
02:22:15	    5	0.114	   0.856
02:23:36	    6	0.109	   0.862
02:24:47	    7	0.110	   0.860
02:25:58	    8	0.103	   0.894
02:27:10	    9	0.101	   0.900
02:28:21	   10	0.100	   0.914
02:29:41	   11	0.098	   0.900
02:30:53	   12	0.097	   0.918
02:32:04	   13	0.096	   0.908
02:33:16	   14	0.097	   0.910
02:34:27	   15	0.098	   0.894
02:35:48	   16	0.095	   0.908
02:36:59	   17	0.095	   0.912
02:38:11	   18	0.094	   0.914
02:39:24	   19	0.093	   0.910
02:40:36	   20	0.093	   0.908
02:41:57	   21	0.094	   0.908
02:43:10	   22	0.094	   0.906
02:44:22	   23	0.093	   0.906
02:45:34	   24	0.093	   0.906
02:46:46	   25	0.092	   0.912
02:48:06	   26	0.091	   0.910
02:49:19	   27	0.093	   0.910
02:50:31	   28	0.091	   0.908
02:51:43	   29	0.091	   0.906
02:52:54	   30	0.090	   0.914

------------------------------

02:53:03	Parameters at epoch 30:

02:53:03	Theta:
02:53:03		 1.2078   0.3605  -1.0923  
02:53:03		 0.5860  -1.1368  -0.4198  
02:53:03		 0.2439   0.9217   1.6361  
02:53:03		-1.0107  -0.8249   0.2802  
02:53:03		 0.4714   0.2712   0.4237  
02:53:03		 0.1277  -2.2795   0.3365  
02:53:03		 0.2955   1.0341   0.2704  
02:53:03		 0.1459  -1.2593   0.1436  
02:53:03	Alpha:
02:53:03		-0.4684   0.7757  
02:53:03		-0.8023   0.9783  
02:53:03		-1.1684   1.1320  
02:53:03		 0.9317   2.6547  
02:53:03		-2.0492   1.1913  
02:53:03		-0.3691  -0.4629  
02:53:03		-1.1707   0.3209  
02:53:03		-0.5059  -0.1140  

------------------------------

02:53:51	Accuracy on test set with the parameters above: 0.8993333333333333

