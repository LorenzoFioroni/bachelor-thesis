03:08:24	Optimizer: adam
03:08:24	Optimizer params: {'lr': 0.02}
03:08:24	DataSet: squares - nTrain: 500 - nValid: 0 - nTest: 3000
03:08:24	nLayers: 3 - nEpochs: 30 - batchSize: 32
03:08:24	Cost: weighted fidelity
03:08:24	Seed: None

------------------------------

03:08:34	epoch	loss	accuracy
03:08:46	    0	0.295	   0.372
03:10:20	    1	0.145	   0.640
03:11:55	    2	0.102	   0.838
03:13:31	    3	0.095	   0.840
03:15:06	    4	0.094	   0.824
03:16:40	    5	0.093	   0.840
03:18:21	    6	0.093	   0.844
03:19:57	    7	0.092	   0.840
03:21:34	    8	0.091	   0.844
03:23:11	    9	0.091	   0.842
03:24:48	   10	0.091	   0.832
03:26:32	   11	0.091	   0.822
03:28:08	   12	0.090	   0.834
03:29:44	   13	0.090	   0.828
03:31:21	   14	0.090	   0.834
03:32:57	   15	0.090	   0.830
03:34:41	   16	0.090	   0.834
03:36:17	   17	0.089	   0.824
03:37:54	   18	0.089	   0.836
03:39:29	   19	0.089	   0.822
03:41:04	   20	0.089	   0.836
03:42:46	   21	0.089	   0.832
03:44:23	   22	0.088	   0.836
03:45:58	   23	0.088	   0.838
03:47:33	   24	0.088	   0.838
03:49:09	   25	0.088	   0.838
03:50:51	   26	0.087	   0.840
03:52:27	   27	0.087	   0.846
03:54:03	   28	0.087	   0.842
03:55:37	   29	0.086	   0.846
03:57:12	   30	0.086	   0.848

------------------------------

03:57:19	Parameters at epoch 30:

03:57:19	Theta:
03:57:19		-1.8705  -1.1467   1.8836  
03:57:19		 0.9893  -1.8053  -0.7668  
03:57:19		 1.2843  -0.3342  -1.5321  
03:57:19	Alpha:
03:57:19		 0.9316  -0.4340  
03:57:19		 1.0414  -0.4838  
03:57:19		-0.8041  -1.4699  
03:57:19	Class weight:
03:57:19		 0.7974   1.0699   0.9065   1.0237  

------------------------------

03:57:55	Accuracy on test set with the parameters above: 0.8136666666666666

