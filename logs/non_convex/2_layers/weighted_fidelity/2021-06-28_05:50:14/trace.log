05:50:14	Optimizer: adam
05:50:14	Optimizer params: {'lr': 0.02}
05:50:14	DataSet: non convex - nTrain: 500 - nValid: 0 - nTest: 3000
05:50:14	nLayers: 2 - nEpochs: 40 - batchSize: 32
05:50:14	Cost: weighted fidelity
05:50:14	Seed: None

------------------------------

05:50:17	epoch	loss	accuracy
05:50:21	    0	0.352	   0.474
05:50:41	    1	0.237	   0.606
05:51:02	    2	0.199	   0.688
05:51:23	    3	0.187	   0.702
05:51:44	    4	0.184	   0.702
05:52:05	    5	0.183	   0.708
05:52:28	    6	0.182	   0.728
05:52:49	    7	0.180	   0.708
05:53:10	    8	0.179	   0.716
05:53:31	    9	0.178	   0.710
05:53:52	   10	0.177	   0.720
05:54:15	   11	0.176	   0.718
05:54:36	   12	0.175	   0.712
05:54:56	   13	0.174	   0.714
05:55:17	   14	0.174	   0.722
05:55:38	   15	0.173	   0.718
05:56:01	   16	0.173	   0.718
05:56:22	   17	0.173	   0.722
05:56:43	   18	0.173	   0.720
05:57:04	   19	0.172	   0.722
05:57:26	   20	0.172	   0.722
05:57:49	   21	0.172	   0.716
05:58:10	   22	0.171	   0.724
05:58:31	   23	0.171	   0.724
05:58:52	   24	0.171	   0.728
05:59:13	   25	0.171	   0.724
05:59:36	   26	0.171	   0.722
05:59:57	   27	0.171	   0.726
06:00:18	   28	0.171	   0.722
06:00:39	   29	0.171	   0.722
06:01:00	   30	0.171	   0.720
06:01:23	   31	0.171	   0.718
06:01:44	   32	0.171	   0.722
06:02:05	   33	0.171	   0.722
06:02:26	   34	0.171	   0.722
06:02:47	   35	0.171	   0.722
06:03:10	   36	0.171	   0.722
06:03:31	   37	0.171	   0.730
06:03:52	   38	0.171	   0.726
06:04:13	   39	0.171	   0.726
06:04:34	   40	0.171	   0.728

------------------------------

06:04:36	Parameters at epoch 40:

06:04:36	Theta:
06:04:36		 1.0409  -0.8348   1.4534  
06:04:36		 0.5237   1.2235  -0.5478  
06:04:36	Alpha:
06:04:36		 0.4289   0.8970  
06:04:36		-0.8129  -0.2031  
06:04:36	Class weight:
06:04:36		 1.0113   1.0087  

------------------------------

06:04:48	Accuracy on test set with the parameters above: 0.7526666666666667

