05:51:33	Optimizer: adam
05:51:33	Optimizer params: {'lr': 0.02}
05:51:33	DataSet: non convex - nTrain: 500 - nValid: 0 - nTest: 3000
05:51:33	nLayers: 2 - nEpochs: 40 - batchSize: 32
05:51:33	Cost: weighted fidelity
05:51:33	Seed: None

------------------------------

05:51:35	epoch	loss	accuracy
05:51:39	    0	0.367	   0.390
05:52:01	    1	0.279	   0.574
05:52:22	    2	0.224	   0.670
05:52:44	    3	0.194	   0.716
05:53:06	    4	0.190	   0.710
05:53:28	    5	0.187	   0.714
05:53:51	    6	0.184	   0.716
05:54:13	    7	0.182	   0.720
05:54:35	    8	0.181	   0.720
05:54:57	    9	0.179	   0.726
05:55:18	   10	0.177	   0.724
05:55:42	   11	0.176	   0.726
05:56:04	   12	0.175	   0.722
05:56:25	   13	0.174	   0.722
05:56:47	   14	0.173	   0.722
05:57:09	   15	0.172	   0.728
05:57:33	   16	0.171	   0.726
05:57:54	   17	0.171	   0.722
05:58:16	   18	0.170	   0.722
05:58:38	   19	0.170	   0.720
05:58:59	   20	0.170	   0.722
05:59:23	   21	0.169	   0.726
05:59:45	   22	0.169	   0.724
06:00:06	   23	0.169	   0.726
06:00:28	   24	0.169	   0.730
06:00:49	   25	0.168	   0.728
06:01:13	   26	0.168	   0.728
06:01:35	   27	0.168	   0.730
06:01:57	   28	0.168	   0.726
06:02:19	   29	0.168	   0.744
06:02:40	   30	0.167	   0.728
06:03:05	   31	0.167	   0.728
06:03:26	   32	0.167	   0.732
06:03:48	   33	0.167	   0.740
06:04:09	   34	0.167	   0.730
06:04:31	   35	0.167	   0.736
06:04:55	   36	0.167	   0.742
06:05:16	   37	0.167	   0.744
06:05:38	   38	0.167	   0.744
06:06:00	   39	0.167	   0.752
06:06:21	   40	0.167	   0.746

------------------------------

06:06:24	Parameters at epoch 40:

06:06:24	Theta:
06:06:24		-0.4673  -0.4299   2.2815  
06:06:24		 1.2654   1.3229  -0.2307  
06:06:24	Alpha:
06:06:24		-0.0025   1.8472  
06:06:24		 1.3090   1.2121  
06:06:24	Class weight:
06:06:24		 1.0393   1.0177  

------------------------------

06:06:35	Accuracy on test set with the parameters above: 0.7706666666666667

