05:50:02	Optimizer: adam
05:50:02	Optimizer params: {'lr': 0.02}
05:50:02	DataSet: non convex - nTrain: 500 - nValid: 0 - nTest: 3000
05:50:02	nLayers: 2 - nEpochs: 40 - batchSize: 32
05:50:02	Cost: weighted fidelity
05:50:02	Seed: None

------------------------------

05:50:05	epoch	loss	accuracy
05:50:11	    0	0.392	   0.498
05:50:34	    1	0.230	   0.678
05:50:59	    2	0.205	   0.726
05:51:22	    3	0.190	   0.716
05:51:46	    4	0.181	   0.724
05:52:10	    5	0.176	   0.734
05:52:36	    6	0.171	   0.744
05:53:00	    7	0.168	   0.734
05:53:24	    8	0.167	   0.738
05:53:47	    9	0.166	   0.744
05:54:11	   10	0.165	   0.742
05:54:37	   11	0.165	   0.744
05:55:01	   12	0.165	   0.748
05:55:25	   13	0.164	   0.742
05:55:49	   14	0.164	   0.746
05:56:14	   15	0.164	   0.746
05:56:41	   16	0.164	   0.744
05:57:06	   17	0.164	   0.744
05:57:29	   18	0.164	   0.746
05:57:53	   19	0.164	   0.744
05:58:17	   20	0.164	   0.746
05:58:43	   21	0.164	   0.740
05:59:07	   22	0.164	   0.746
05:59:30	   23	0.164	   0.748
05:59:54	   24	0.164	   0.746
06:00:18	   25	0.164	   0.742
06:00:44	   26	0.164	   0.746
06:01:09	   27	0.164	   0.740
06:01:33	   28	0.164	   0.748
06:01:57	   29	0.164	   0.746
06:02:21	   30	0.164	   0.746
06:02:48	   31	0.164	   0.750
06:03:12	   32	0.164	   0.746
06:03:36	   33	0.164	   0.744
06:04:00	   34	0.164	   0.742
06:04:24	   35	0.164	   0.746
06:04:50	   36	0.164	   0.746
06:05:14	   37	0.164	   0.744
06:05:39	   38	0.164	   0.746
06:06:03	   39	0.164	   0.742
06:06:26	   40	0.164	   0.746

------------------------------

06:06:29	Parameters at epoch 40:

06:06:29	Theta:
06:06:29		-1.7416   0.3818   2.4708  
06:06:29		 2.2525   1.7057   0.8233  
06:06:29	Alpha:
06:06:29		 0.3935   1.0356  
06:06:29		-0.9621   0.4341  
06:06:29	Class weight:
06:06:29		 1.0620   0.9607  

------------------------------

06:06:41	Accuracy on test set with the parameters above: 0.7466666666666667

