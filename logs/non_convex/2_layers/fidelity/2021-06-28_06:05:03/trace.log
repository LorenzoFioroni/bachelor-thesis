06:05:03	Optimizer: adam
06:05:03	Optimizer params: {'lr': 0.02}
06:05:03	DataSet: non convex - nTrain: 500 - nValid: 0 - nTest: 3000
06:05:03	nLayers: 2 - nEpochs: 40 - batchSize: 32
06:05:03	Cost: fidelity
06:05:03	Seed: None

------------------------------

06:05:06	epoch	loss	accuracy
06:05:09	    0	0.462	   0.588
06:05:21	    1	0.376	   0.716
06:05:32	    2	0.330	   0.738
06:05:44	    3	0.310	   0.736
06:05:55	    4	0.304	   0.734
06:06:06	    5	0.301	   0.734
06:06:20	    6	0.300	   0.732
06:06:32	    7	0.299	   0.738
06:06:43	    8	0.298	   0.738
06:06:55	    9	0.298	   0.738
06:07:06	   10	0.298	   0.730
06:07:19	   11	0.297	   0.734
06:07:31	   12	0.297	   0.736
06:07:42	   13	0.297	   0.736
06:07:54	   14	0.297	   0.734
06:08:05	   15	0.297	   0.732
06:08:19	   16	0.297	   0.732
06:08:30	   17	0.296	   0.732
06:08:42	   18	0.296	   0.732
06:08:53	   19	0.296	   0.734
06:09:04	   20	0.296	   0.730
06:09:18	   21	0.296	   0.734
06:09:30	   22	0.296	   0.730
06:09:41	   23	0.296	   0.734
06:09:52	   24	0.296	   0.730
06:10:04	   25	0.296	   0.730
06:10:18	   26	0.296	   0.730
06:10:29	   27	0.296	   0.730
06:10:40	   28	0.296	   0.732
06:10:52	   29	0.296	   0.730
06:11:03	   30	0.296	   0.734
06:11:17	   31	0.296	   0.732
06:11:29	   32	0.296	   0.730
06:11:40	   33	0.296	   0.730
06:11:51	   34	0.296	   0.730
06:12:03	   35	0.296	   0.730
06:12:16	   36	0.296	   0.730
06:12:28	   37	0.296	   0.730
06:12:39	   38	0.296	   0.730
06:12:50	   39	0.296	   0.730
06:13:02	   40	0.296	   0.730

------------------------------

06:13:04	Parameters at epoch 40:

06:13:04	Theta:
06:13:04		 0.4648   0.8880  -0.6490  
06:13:04		-0.0712   0.7902   1.2746  
06:13:04	Alpha:
06:13:04		-0.9485  -1.9320  
06:13:04		-0.2620  -0.1714  

------------------------------

06:13:15	Accuracy on test set with the parameters above: 0.7466666666666667

